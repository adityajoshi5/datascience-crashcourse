{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba79842",
   "metadata": {},
   "source": [
    "# Practical Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc63b7d",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "In this part of the day we will be building a machine learning algorithm together with Python and the library scikit-learn.\n",
    "\n",
    "Our team prepared a dataset which consists of 100 instances with the granularity of a single student. This dataset is an aggregation from the logs of an e-learning system with learning material and exams. \n",
    "\n",
    "Our goal today is to build a model to predict, given the actions of a student, whether the student will pass or fail the final exam (column = FinalResult).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4cc0fa",
   "metadata": {},
   "source": [
    "## scikit-learn\n",
    "scikit-learn is one of the most prominent Python libraries for machine learning:\n",
    "\n",
    "* Contains many state-of-the-art machine learning algorithms\n",
    "* Wide range of evaluation measures and techniques\n",
    "* Offers comprehensive documentation about each algorithm\n",
    "* Widely used, and a wealth of tutorials and code snippets are available\n",
    "* Works well with numpy, scipy, pandas, matplotlib,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Global imports and settings\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b768f69",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fb2e6",
   "metadata": {},
   "source": [
    "If you have the data loaded in a pandas file, then this part is not needed. If not, we first have to load the data. \n",
    "\n",
    "* Load the \"flatfile 1.csv\" into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = #Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc1f6e",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da48ef",
   "metadata": {},
   "source": [
    "In part 1 today you've already worked with the data and performed an Explorative Data Analysis. This is often a crucial part of the process, since you need to understand what you are working with before editing it. You can do some exploring here to see which variables are in the data and how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples\n",
    "\n",
    "# data.shape\n",
    "\n",
    "# data.describe()\n",
    "\n",
    "# data[\"UnqVideosWatched\"][0:50]\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f1a6b",
   "metadata": {},
   "source": [
    "## 2. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380dc39",
   "metadata": {},
   "source": [
    "The data first needs to be processed before we will train a model on the data. Some models only accept data in a certain way. Lets make our data model-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ed582",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "The first thing to do when processing your data is to check for missing values. The easy way is to do this with pandas.\n",
    "\n",
    "* the pandas .isnull() command on a dataframe returns the whole dataframe, but checks for every number if its empty or not and returns a Boolean.\n",
    "* if you sum all those booleans, you will find the amount of rows that are empty for a certain column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing values there are\n",
    "\n",
    "data.#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b74a7",
   "metadata": {},
   "source": [
    "We need to decide what to do with the missing data. \n",
    "\n",
    "For this scenario, where a specific test is not made at all, it does not make sense to apply the average score to it. It will most likely distort the data distribution of these test scores. Also, the fact that someone did not make a test also gives valuable information. By imputing an average, you will lose information and introduce a bias.\n",
    "\n",
    "* Lets decide to put a 0 for a test that is not done, to keep the information of a non-taken test. This can be done by the .fillna() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all empty values to 0's \n",
    "\n",
    "data.#Your code here\n",
    "\n",
    "# Check if there are missing values left\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c756045",
   "metadata": {},
   "source": [
    "### Binary encoding\n",
    "\n",
    "As you may have seen, the FinalResult column is a binary value defined with a \"Pass\" or \"Fail\". For machine learning, it's easier to model this into a 1 or 0 column instead.\n",
    "\n",
    "* With pandas, you need to filter all the rows with FinalResult == \"Pass\" and turn them into 1's. \n",
    "* Same for \"Fail\", but turn those into 0's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5379912",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FinalResult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FinalResult'] = data[\"FinalResult\"].replace(#Your code here\n",
    "data['FinalResult'] = data[\"FinalResult\"].replace(#Your code here\n",
    "data['FinalResult']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f112b5",
   "metadata": {},
   "source": [
    "### Additional features\n",
    "\n",
    "We can come up with all kinds of different features. Lets add at least 2 more.\n",
    "\n",
    "* The average score of all quizzes combined (AvgQuizGrade). Using the mean() function with axis=1 as a parameter on a column list, you will get the mean of the values in the columns for each row. Another option is to sum all the quiz values, and divide it by the amount of quizzes. \n",
    "\n",
    "*Question:* Why would we use one over the other?\n",
    "\n",
    "\n",
    "* An overal \"activity\" score: combinding UnqArticlesRead and UnqVideosWatched\n",
    "* Optional: Getting features from the date column: Weekday/Weekend/DaysFromNow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca07425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AvgQuizGrade\n",
    "quiz_column_list = ['Quiz1', 'Quiz2', 'Quiz3', 'Quiz4', 'Quiz5', 'Quiz6', 'Quiz7', 'Quiz8', 'Quiz9', 'Quiz10']\n",
    "\n",
    "data[\"AvgQuizGrade\"] = data[quiz_column_list].#Your code here\n",
    "data[\"AvgQuizGrade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OveralActivityScore\n",
    "\n",
    "data[\"OveralActivityScore\"] = #Your code here\n",
    "data[\"OveralActivityScore\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e81074",
   "metadata": {},
   "source": [
    "### Feature engineering with dates\n",
    "\n",
    "The FirstLoginDate is currently read as a string. We first need to convert it to a datetime object and can then apply the transformations we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date columns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "data[\"FirstLoginDate\"] = pd.to_datetime(data[\"FirstLoginDate\"])\n",
    "data[\"FirstLoginDate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12262a6",
   "metadata": {},
   "source": [
    "Using .dt.weekday (the dt stands for datetime) on your data column will return the weekday number from [0-7]. Apply this to check whether the day is a weekday or weekend and make these 2 new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b425311",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WeekdayLoginDate'] = #Your code here\n",
    "data['WeekendLoginDate'] = #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabf1ac",
   "metadata": {},
   "source": [
    "### Days till now\n",
    "\n",
    "Lets assume the exam was today. Students who logged in for the first time late might not have had enough time or be motivated enough to study. Thus, with this hypothesis, we can calculate the days from now to get information about the motivation of a student.\n",
    "\n",
    "* Subtract from the current date the date the first login happened. Then, do a .dt.days to get the amount of days.\n",
    "* Lets also drop the FirstLoginDate column, since it's not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now()\n",
    "\n",
    "data['days_till_now'] = #Your code here\n",
    "\n",
    "data = data.drop('FirstLoginDate', axis=1)\n",
    "\n",
    "data['days_till_now']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c369b66",
   "metadata": {},
   "source": [
    "## Splitting your data into X and Y\n",
    "\n",
    "In machine learning, you use a certain part of your data as features, and certain part of your data as the predicted value. Those we call X and Y respectively.\n",
    "\n",
    "* Split a subset of your data into the X variable: all the other columns, except the target. In this case it's 2 variables: 'FinalResult', 'CourseGrade'. \n",
    "* Assign the output/target column to the y variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c42b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "\n",
    "X = #Your code\n",
    "\n",
    "y = #Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a9271",
   "metadata": {},
   "source": [
    "## 3. Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f515362",
   "metadata": {},
   "source": [
    "Lets suppose all the data we have are these 100 instances. \n",
    "\n",
    "We cannot use all of the 100 instances to train the model, since then we will have no \"unseen\" instance to test our model on. Thus, we need to split the data. One part will be a training set, and another part will be the test set.\n",
    "\n",
    "`train_test_split` from sklearn: splits data randomly in X% training and Y% test data. \n",
    "\n",
    "* What % should we set the test_size at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad379e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = #Your % here in 0.X format.  \n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9142a3",
   "metadata": {},
   "source": [
    "## 4. Model selection & training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1383b",
   "metadata": {},
   "source": [
    "All scikitlearn estimators follow the same interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a0fbe",
   "metadata": {},
   "source": [
    "```python\n",
    "class SupervisedEstimator(...):\n",
    "    def __init__(self, hyperparam, ...):\n",
    "\n",
    "    def fit(self, X, y):   # Fit/model the training data\n",
    "        ...                # given data X and targets y\n",
    "        return self\n",
    "     \n",
    "    def predict(self, X):  # Make predictions\n",
    "        ...                # on unseen data X  \n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y): # Predict and compare to true\n",
    "        ...                # labels y                \n",
    "        return score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd060c",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "Selecting a model is an art in itself. It's a balance between complexity, interpretebility, available resources, and trial-and-error.\n",
    "\n",
    "Some examples for classification:\n",
    "\n",
    "* Logistic Regression: Good for binary classification problems, especially as a baseline model.\n",
    "\n",
    "* Decision Trees: Useful for interpretability and when handling categorical features.\n",
    "\n",
    "* Random Forest and Ensemble Methods: Offer improvements over single decision trees, reducing the risk of overfitting.\n",
    "\n",
    "* Support Vector Machines (SVM): Effective in high-dimensional spaces and with various kernel functions.\n",
    "\n",
    "* Neural Networks: Suitable for complex problems, particularly with large amounts of data and high computational capacity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e3cae",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "\n",
    "The first model we'll build is one of the most interpretable models there are. We are going to train a Decision-tree classifier.\n",
    "\n",
    "When building ML systems, we are not actually telling the system how it should classify the instances. We are giving it data, providing the output, and asking the code to give us it's best guess formula to get there. \n",
    "\n",
    "If you would do this by hand, you will look at the data, create if/else statements, and split the data untill you get something that gives you a decent enough answer. This is kind of what the decision tree classifier is also doing. The output is an if/else structure which is easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23566237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the model\n",
    "model = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906004a7",
   "metadata": {},
   "source": [
    "Now we have an empty model. We need to train it using model.fit and giving it our data X_train and giving it the targets of that data y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f10fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(# Your code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810f639",
   "metadata": {},
   "source": [
    "Now we have a trained model!\n",
    "\n",
    "Because we just have 100 instances and it's a simple classifier, this happend quickly. With complex models, with many nodes (neural networks), and a lot of data this can take months with specialized hardware (TPU's).\n",
    "\n",
    "We can now create a random instance and predict if it will pass (1) or fail (0) a test. \n",
    "\n",
    "Play with the variables in this instance to check when it will predict a fail (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebef060",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_instance = pd.DataFrame(columns=X.columns)\n",
    "random_instance.loc[0] = [8246314, 22, 8, 23, 43, 17, 9, 4, 56, 25, 71, 70, 100, 84, 67, 54, 44, 61, 0, 72, 62.3, 60, True, True, 267]\n",
    "random_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed40686",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(random_instance)\n",
    "prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f150c",
   "metadata": {},
   "source": [
    "*hint* a suspect might be \"QuizReviewed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77be0d1",
   "metadata": {},
   "source": [
    "# Break: 10 minutes coffee/toilet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227bbbb",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9b311",
   "metadata": {},
   "source": [
    "### Predict all test examples\n",
    "\n",
    "Instead of giving it just 1 example, give it all the X_test examples as input to the .predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c27819",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(#Your code here)\n",
    "    \n",
    "print(\"Test set predictions:\\n {}\".format(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3b9b1",
   "metadata": {},
   "source": [
    "The score function computes the percentage of correct predictions. This is also called the Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0d234",
   "metadata": {},
   "source": [
    "### Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfc882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score: {:.2f}\".format(model.score(X_test, y_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c52b32",
   "metadata": {},
   "source": [
    "### Other evaluation metrics\n",
    "\n",
    "Accuracy is not enough to judge a model's performance. There are more metrics which say something about the performance of the model. They all use the confusion matrix as a basis. \n",
    "\n",
    "Trivia: \"The name [confusion matrix] stems from the fact that it makes it easy to see whether the system is confusing two classes\"\n",
    "\n",
    "Lets calculate the confusion matrix and the evaluation metrics.\n",
    "\n",
    "* The sklearn function to use is \"confusion matrix\" which needs the inputs of your test set targets (y_test) and predictions (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "conf_matrix = #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0138b8c",
   "metadata": {},
   "source": [
    "Lets plot the confusion matrix so you can see how your model performed with classifying the test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa079505",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba58608",
   "metadata": {},
   "source": [
    "*Question*: Can you interpret this plot? What does it say?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51ac67",
   "metadata": {},
   "source": [
    "## Precision & Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe81db7",
   "metadata": {},
   "source": [
    "Precision measures the proportion of correctly predicted positive observations to the total predicted positives. It answers the question: \"Of all instances classified as positive, how many are actually positive?\"\n",
    "\n",
    "Formula: Precision = True Positives / (True Positives + False Positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e567f",
   "metadata": {},
   "source": [
    "Recall (or Sensitivity) measures the proportion of correctly predicted positive observations to all observations in the actual class. It answers the question: \"Of all actual positives, how many were correctly identified?\"\n",
    "\n",
    "Formula: Recall = True Positives / (True Positives + False Negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed9c6e",
   "metadata": {},
   "source": [
    "Calculate the Precision and recall yourself. You can use sklearn imports, or calculate them by using the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385eba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precsion and Recall\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d76f9e",
   "metadata": {},
   "source": [
    "## F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a991e8",
   "metadata": {},
   "source": [
    " The F1 score is the harmonic mean of precision and recall. It is a better measure than accuracy for imbalanced datasets.\n",
    " \n",
    "\\begin{equation}\n",
    "    F1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{equation}\n",
    "\n",
    "Calculate the F1 score using sklearn imports or manually using precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F1 score\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e1537",
   "metadata": {},
   "source": [
    "## Visualizing the trained decision tree\n",
    "\n",
    "You've trained a decision tree classifier which can predict instances. But how does it do this? \n",
    "\n",
    "The most important quality of a decision tree is that it's interpretable. We can plot the tree that the model is using to classify instances. \n",
    "\n",
    "* The top node is called the \"root\". The root node is often considered the most important feature in rapport with all other features.\n",
    "* The decisions are called \"branches\"\n",
    "* The end nodes are called \"leafs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bcd4ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "tree.plot_tree(model, feature_names=X.columns, fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca020a06",
   "metadata": {},
   "source": [
    "Again using the confusion matrix we can derive other metrics like the True Positive Rate or the False Positive Rate.\n",
    "\n",
    "The False Positive Rate is the number of false positives divided by the total number of actual negatives, while the True Positive Rate is the number of true positives divided by the total number of actual positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db377af",
   "metadata": {},
   "source": [
    "The True Positive Rate (TPR), also known as sensitivity or recall, is calculated as:\n",
    "\\begin{equation}\n",
    "    TPR = \\frac{TP}{TP + FN}\n",
    "\\end{equation}\n",
    "where \\( TP \\) represents the number of true positives and \\( FN \\) represents the number of false negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbaa67",
   "metadata": {},
   "source": [
    "The False Positive Rate (FPR) is calculated as:\n",
    "\\begin{equation}\n",
    "    FPR = \\frac{FP}{FP + TN}\n",
    "\\end{equation}\n",
    "where \\( FP \\) represents the number of false positives and \\( TN \\) represents the number of true negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae74ac1",
   "metadata": {},
   "source": [
    "## AUC-ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0e56e",
   "metadata": {},
   "source": [
    "The AUC-ROC curve is a performance measurement for the classification problems at various threshold settings. \n",
    "\n",
    "ROC (Receiver Operating Characteristics) is a probability curve, \n",
    "\n",
    "and AUC (Area Under the Curve) represents the degree or measure of separability between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad63f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a972f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867369b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], color='darkgray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f027a",
   "metadata": {},
   "source": [
    "AUC is a useful metric to compare your model to other models and works well for imbalanced datasets. It measures how well the model discriminates between positive and negative classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ec20a",
   "metadata": {},
   "source": [
    "## 6. Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b15d7",
   "metadata": {},
   "source": [
    "The above classifier/estimator, the DecisionTreeClassifier, is trained using the most default parameters. Check the documentation of this classifier on the website of sci-kit learn:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "You will see that this classifier has more than 10 parameters that you can pick and decide upon. For examples the parameters \"max_depth\" or \"criterion\" or \"min_samples_leaf\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd556b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=15, n_jobs=-1, verbose=2, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb49a52",
   "metadata": {},
   "source": [
    "*Question:* How did it come up with 162 candidates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6725e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e5c52",
   "metadata": {},
   "source": [
    "## 7. Model refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best parameters\n",
    "refined_model = DecisionTreeClassifier(**best_params, random_state=0)\n",
    "\n",
    "# Train the model\n",
    "refined_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the refined model\n",
    "accuracy_refined_model = refined_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy score of refined model: {:.2f}\".format(accuracy_refined_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af21090",
   "metadata": {},
   "source": [
    "*Question:* Do you know what cv stands for in GridSearchCV?\n",
    "\n",
    "*Question:* Can you think why the refined model might give a lower accuracy than our basic model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cdcbc",
   "metadata": {},
   "source": [
    "### Goal: generalizability vs accuracy. Overfitting vs underfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37cbdc0",
   "metadata": {},
   "source": [
    "## Optional: Repeat above steps, but for a different classifier, and evaluate which is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fba048",
   "metadata": {},
   "source": [
    "Explain why 'CourseGrade' is taken out as a variable, and how it can be used for Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7563a",
   "metadata": {},
   "source": [
    "Try one of the following:\n",
    "* Linear regression\n",
    "* Random forest\n",
    "* Support Vector Machines\n",
    "\n",
    "Look up the documentation in sklearn how to use them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
